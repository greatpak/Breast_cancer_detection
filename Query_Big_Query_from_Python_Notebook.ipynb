{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greatpak/Breast_cancer_detection/blob/master/Query_Big_Query_from_Python_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-cloud-bigquery"
      ],
      "metadata": {
        "id": "n4LvC5cZK3x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "metadata": {
        "id": "mrdyqZ1CKTcD",
        "outputId": "b97cbf2c-677a-490e-d9d6-ca72133d3f7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your Google Cloud Project ID\n",
        "project_id = 'sqlcourse-352110'"
      ],
      "metadata": {
        "id": "z3T36Uz5O1gS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Create a BigQuery client\n",
        "client = bigquery.Client(project=project_id)"
      ],
      "metadata": {
        "id": "A842uhabLhK1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "          SELECT name, SUM(number) as count\n",
        "          FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
        "          GROUP BY name\n",
        "          ORDER BY count DESC\n",
        "          LIMIT 10\n",
        "        \"\"\"\n",
        "\n",
        "query_job = client.query(query)"
      ],
      "metadata": {
        "id": "7wObBCmuOQWW",
        "outputId": "d615399a-3fef-4cb7-fc86-3b35acfdcd18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'google.cloud.bigquery' has no attribute 'retry'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1336709530.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \"\"\"\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mquery_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[1;32m   3571\u001b[0m             )\n\u001b[1;32m   3572\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mapi_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueryApiMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINSERT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3573\u001b[0;31m             return _job_helpers.query_jobs_insert(\n\u001b[0m\u001b[1;32m   3574\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3575\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mquery_jobs_insert\u001b[0;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, callback)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# continue to do so by setting job_retry to None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjob_retry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mdo_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DEFAULT_QUERY_JOB_INSERT_RETRY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'google.cloud.bigquery' has no attribute 'retry'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = query_job.to_dataframe()\n",
        "df"
      ],
      "metadata": {
        "id": "8DKwm7WyOTqY",
        "outputId": "d55972fc-11a8-4e6f-fe11-91c990161710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'query_job' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1244797782.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'query_job' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import psycopg2\n",
        "from psycopg2.extras import execute_values\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "from sshtunnel import SSHTunnelForwarder\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ Load secrets from .env file\n",
        "# ===============================\n",
        "load_dotenv()  # will read .env file if present\n",
        "\n",
        "REDSHIFT_CONFIG = {\n",
        "    \"database\": os.getenv(\"REDSHIFT_DB\"),\n",
        "    \"user\": os.getenv(\"REDSHIFT_USER\"),\n",
        "    \"password\": os.getenv(\"REDSHIFT_PASSWORD\"),\n",
        "    \"host\": os.getenv(\"REDSHIFT_HOST\"),\n",
        "    \"port\": int(os.getenv(\"REDSHIFT_PORT\", 5439))\n",
        "}\n",
        "\n",
        "SSH_CONFIG = {\n",
        "    \"ssh_host\": os.getenv(\"SSH_HOST\"),\n",
        "    \"ssh_port\": int(os.getenv(\"SSH_PORT\", 22)),\n",
        "    \"ssh_user\": os.getenv(\"SSH_USER\"),\n",
        "    \"ssh_password\": os.getenv(\"SSH_PASSWORD\"),\n",
        "}\n",
        "\n",
        "BQ_PROJECT_ID = os.getenv(\"BQ_PROJECT_ID\", \"gtm-mkxz6jtz-ndzln\")\n",
        "\n",
        "BQ_QUERY = \"\"\"\n",
        "    SELECT *\n",
        "    FROM `indigo-codex-393317.matw_international_google_ads.ads_AccountBasicStats_7713128947`\n",
        "\"\"\"\n",
        "\n",
        "REDSHIFT_SCHEMA = \"bronze\"\n",
        "REDSHIFT_TABLE = \"stg_ga_matw_international_google_ads\"\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ Fetch data from BigQuery\n",
        "# ===============================\n",
        "client = bigquery.Client(project=BQ_PROJECT_ID)\n",
        "query_job = client.query(BQ_QUERY)\n",
        "df = query_job.to_dataframe()\n",
        "\n",
        "print(f\"âœ… Pulled {len(df)} rows from BigQuery\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ Function: Insert DataFrame into Redshift (via SSH Tunnel)\n",
        "# ===============================\n",
        "def insert_dataframe_to_redshift(df: pd.DataFrame, schema: str, table: str):\n",
        "    \"\"\"Insert DataFrame into Redshift using SSH tunnel + psycopg2 batch insert\"\"\"\n",
        "\n",
        "    # Convert dict & list columns into strings\n",
        "    for column in df.columns:\n",
        "        if df[column].apply(lambda x: isinstance(x, dict)).any():\n",
        "            df[column] = df[column].apply(lambda x: json.dumps(x) if isinstance(x, dict) else x)\n",
        "\n",
        "        if df[column].apply(lambda x: isinstance(x, list)).any():\n",
        "            df[column] = df[column].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else x)\n",
        "\n",
        "    # Replace NaN/None with None\n",
        "    df = df.where(pd.notnull(df), None)\n",
        "\n",
        "    with SSHTunnelForwarder(\n",
        "        (SSH_CONFIG[\"ssh_host\"], SSH_CONFIG[\"ssh_port\"]),\n",
        "        ssh_username=SSH_CONFIG[\"ssh_user\"],\n",
        "        ssh_password=SSH_CONFIG[\"ssh_password\"],\n",
        "        remote_bind_address=(REDSHIFT_CONFIG[\"host\"], REDSHIFT_CONFIG[\"port\"]),\n",
        "        local_bind_address=(\"localhost\", 5439)\n",
        "    ) as tunnel:\n",
        "        print(\"âœ… SSH tunnel established\")\n",
        "\n",
        "        # Redshift connection through tunnel\n",
        "        conn = psycopg2.connect(\n",
        "            dbname=REDSHIFT_CONFIG[\"database\"],\n",
        "            user=REDSHIFT_CONFIG[\"user\"],\n",
        "            password=REDSHIFT_CONFIG[\"password\"],\n",
        "            host=\"localhost\",\n",
        "            port=tunnel.local_bind_port\n",
        "        )\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Prepare INSERT SQL\n",
        "        columns = ', '.join(f'\"{col}\"' for col in df.columns)\n",
        "        insert_sql = f'INSERT INTO \"{schema}\".\"{table}\" ({columns}) VALUES %s'\n",
        "\n",
        "        # Convert DataFrame to list of tuples\n",
        "        rows = list(df.itertuples(index=False, name=None))\n",
        "\n",
        "        # Batch insert\n",
        "        batch_size = 5000\n",
        "        for i in range(0, len(rows), batch_size):\n",
        "            batch = rows[i:i + batch_size]\n",
        "            execute_values(cursor, insert_sql, batch)\n",
        "            conn.commit()\n",
        "            print(f\"Inserted {min(i + batch_size, len(rows))}/{len(rows)} rows\")\n",
        "\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "        print(f\"âœ… Finished inserting {len(rows)} rows into {schema}.{table}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ðŸ”¹ Run Insert\n",
        "# ===============================\n",
        "insert_dataframe_to_redshift(df, REDSHIFT_SCHEMA, REDSHIFT_TABLE)\n"
      ],
      "metadata": {
        "id": "B1Lgc64LRW_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GMroggFISQsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}